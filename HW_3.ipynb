{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание по теме урока: Логистическая регрессия. Log Loss.\n",
    "**Постараюсь реализовать многоклассовую логрегрессию (частный случай которой - 2 класса**)\n",
    "\n",
    "***Соответствующими комметариями буду указывать на реализации кода, которые соответствуют номерам ДЗ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "              [1, 1, 2, 5, 3, 0, 5, 10, 1, 2],\n",
    "              [500, 700, 750, 600, 1450, 800, 1500, 2000, 450, 1000],\n",
    "              [1, 1, 2, 1, 2,  1, 3, 3, 1, 2]], dtype = np.float64)\n",
    "y = np.array([0, 0, 1, 0, 1, 0, 1, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_std_feat(x):\n",
    "    res = (x - x.mean()) / x.std()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_st = X.copy()\n",
    "X_st[2, :] = calc_std_feat(X[2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ,  2.        ,  5.        ,  3.        ,\n",
       "         0.        ,  5.        , 10.        ,  1.        ,  2.        ],\n",
       "       [-0.97958969, -0.56713087, -0.46401617, -0.77336028,  0.97958969,\n",
       "        -0.36090146,  1.08270439,  2.11385144, -1.08270439,  0.05155735],\n",
       "       [ 1.        ,  1.        ,  2.        ,  1.        ,  2.        ,\n",
       "         1.        ,  3.        ,  3.        ,  1.        ,  2.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''функция для преобразования вектора индексов классов в матрицу, содержащий\n",
    "one-hot веткор для каждого экземпляра. В нашем случае это два экземпляра,\n",
    "но данная реализация может быть использованап по множество экземпляров'''\n",
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_one_hot = to_one_hot(y)\n",
    "Y_train_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#реализация функции softmax (как в scipy.special) (оно же, по сути, №3 домашки)\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определим количество входов и выходов для обучения\n",
    "n_inputs = X_st.T.shape[1] # == 4 (3 признака плюс смещение)\n",
    "n_outputs = len(np.unique(y))   # == 2 класса\n",
    "n_inputs, n_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Измените функцию calc_logloss так, чтобы нули по возможности не попадали в np.log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*при реализации просто добавлю epsilon = 1e-7, это избавит как от нулей, так и от nan*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.7017819923396322\n",
      "500 0.5274229425235799\n",
      "1000 0.47240370090616135\n",
      "1500 0.43704164597316264\n",
      "2000 0.4096114369949804\n",
      "2500 0.38765445447671254\n",
      "3000 0.3698383448531706\n",
      "3500 0.35519429215605125\n",
      "4000 0.3429920436782424\n",
      "4500 0.33268336530321674\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 5000\n",
    "m = len(X_st)\n",
    "epsilon = 1e-7\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_st.T.dot(Theta)#транспонирую X_st (здесь везде далее), чтобы соответствовать стандартным формулам\n",
    "    Y_proba = softmax(logits)\n",
    "    loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, loss)\n",
    "    gradients = 1/m * X_st.dot(error)\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.38073559, -2.016056  ],\n",
       "       [ 1.11844771,  0.13294873],\n",
       "       [ 0.14804978, -0.170559  ],\n",
       "       [-2.04369481,  2.29935959]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.67417522, 0.32582478],\n",
       "        [0.70235622, 0.29764378],\n",
       "        [0.07826488, 0.92173512],\n",
       "        [0.99129257, 0.00870743],\n",
       "        [0.26488827, 0.73511173],\n",
       "        [0.48469249, 0.51530751],\n",
       "        [0.0335719 , 0.9664281 ],\n",
       "        [0.8694506 , 0.1305494 ],\n",
       "        [0.6669177 , 0.3330823 ],\n",
       "        [0.09096634, 0.90903366]]),\n",
       " array([0, 0, 1, 0, 1, 1, 1, 0, 0, 1], dtype=int64))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ДЗ 3 и 4\n",
    "logits = X_st.T.dot(Theta)\n",
    "Y_proba = softmax(logits) # это есть номер 3 домашки\n",
    "y_predict = np.argmax(Y_proba, axis=1) # это есть номер 4 домашки\n",
    "Y_proba,y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_Actual  y_Predicted\n",
      "0         0            0\n",
      "1         0            0\n",
      "2         1            1\n",
      "3         0            0\n",
      "4         1            1\n",
      "5         0            1\n",
      "6         1            1\n",
      "7         0            0\n",
      "8         1            0\n",
      "9         1            1\n"
     ]
    }
   ],
   "source": [
    "# ДЗ 5\n",
    "data = {'y_Actual':    y,\n",
    "        'y_Predicted': y_predict\n",
    "        }\n",
    "df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  0  1\n",
      "Actual         \n",
      "0          4  1\n",
      "1          1  4\n"
     ]
    }
   ],
   "source": [
    "# ДЗ 5\n",
    "data = {'y_Actual':    y,\n",
    "        'y_Predicted': y_predict\n",
    "        }\n",
    "df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ДЗ 5\n",
    "accuracy = (confusion_matrix.iloc[1, 1]+confusion_matrix.iloc[0, 0])/(confusion_matrix.iloc[0, 0]+confusion_matrix.iloc[1, 0]+confusion_matrix.iloc[0, 1]+confusion_matrix.iloc[1, 1])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.8, 0.8000000000000002)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ДЗ 5\n",
    "Precission = confusion_matrix.iloc[1, 1]/(confusion_matrix.iloc[1, 1] + confusion_matrix.iloc[0, 1])\n",
    "Recall = confusion_matrix.iloc[1, 1]/(confusion_matrix.iloc[1, 1] + confusion_matrix.iloc[1, 0])\n",
    "F = (2*Precission*Recall)/(Precission+Recall)\n",
    "Precission, Recall, F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ 2 и 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Да модель могла переобучиться, потому что мы никак не ограничивали наши веса. Ниже я делаю реализацию регуляризации, а также прекращения градиентного спуска во избежание лишних иттераций (что соответствкет ДЗ 2 по сути)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.2087298020618418\n",
      "10 1.0807998509835701\n",
      "20 0.9562263404698128\n",
      "30 0.891962784827398\n",
      "40 0.8440131668735087\n",
      "50 0.803742739138587\n",
      "60 0.7689964752745102\n",
      "70 0.7382196134461858\n",
      "80 0.7108012267336091\n",
      "90 0.6864335814151389\n",
      "100 0.664872015667269\n",
      "110 0.6458910925797154\n",
      "120 0.6292721887092488\n",
      "130 0.6148014086399369\n",
      "140 0.6022714181596752\n",
      "150 0.5914841153897332\n",
      "160 0.5822529149646493\n",
      "170 0.5744042897626065\n",
      "180 0.5677785728118444\n",
      "190 0.5622301479132764\n",
      "200 0.557627183581764\n",
      "210 0.5538510514016974\n",
      "220 0.5507955437939692\n",
      "230 0.5483659790550671\n",
      "240 0.5464782577766192\n",
      "250 0.5450579155871145\n",
      "260 0.5440392024156243\n",
      "270 0.5433642075062665\n",
      "280 0.5429820414570716\n",
      "290 0.5428480809237464\n",
      "291 0.5428467879995694\n",
      "292 0.5428475463937765 early stopping!\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 \n",
    "n_iterations = 10001\n",
    "m = len(X_st)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1  # гиперпараметр регуляризации\n",
    "best_loss = np.infty\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_st.T.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "    loss = xentropy_loss + alpha * l2_loss\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_st.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients\n",
    "\n",
    "    logits = X_st.T.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "    loss = xentropy_loss + alpha * l2_loss\n",
    "    if iteration % 10 == 0:\n",
    "        print(iteration, loss)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        print(iteration - 1, best_loss)\n",
    "        print(iteration, loss, \"early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17679294, -1.06071302],\n",
       "       [ 0.31553991, -0.34992096],\n",
       "       [-0.24587814,  0.23122964],\n",
       "       [-0.92169567,  0.97234298]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.61687333, 0.38312667],\n",
       "        [0.56942627, 0.43057373],\n",
       "        [0.2692829 , 0.7307171 ],\n",
       "        [0.95433784, 0.04566216],\n",
       "        [0.26472392, 0.73527608],\n",
       "        [0.38122693, 0.61877307],\n",
       "        [0.16330058, 0.83669942],\n",
       "        [0.76878507, 0.23121493],\n",
       "        [0.62843175, 0.37156825],\n",
       "        [0.22369742, 0.77630258]]),\n",
       " array([0, 0, 1, 0, 1, 1, 1, 0, 0, 1], dtype=int64))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_st.T.dot(Theta)\n",
    "Y_proba = softmax(logits) # это есть номер 3 домашки\n",
    "y_predict = np.argmax(Y_proba, axis=1) # это есть номер 4 домашки\n",
    "Y_proba,y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  0  1\n",
      "Actual         \n",
      "0          4  1\n",
      "1          1  4\n"
     ]
    }
   ],
   "source": [
    "data = {'y_Actual':    y,\n",
    "        'y_Predicted': y_predict\n",
    "        }\n",
    "df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (confusion_matrix.iloc[1, 1]+confusion_matrix.iloc[0, 0])/(confusion_matrix.iloc[0, 0]+confusion_matrix.iloc[1, 0]+confusion_matrix.iloc[0, 1]+confusion_matrix.iloc[1, 1])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.8, 0.8000000000000002)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precission = confusion_matrix.iloc[1, 1]/(confusion_matrix.iloc[1, 1] + confusion_matrix.iloc[0, 1])\n",
    "Recall = confusion_matrix.iloc[1, 1]/(confusion_matrix.iloc[1, 1] + confusion_matrix.iloc[1, 0])\n",
    "F = (2*Precission*Recall)/(Precission+Recall)\n",
    "Precission, Recall, F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**результаты аналогичны (так получилось, нюанс этой мизернйо выборки) а модель работала быстрее и веса меньше изза регуляризации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
